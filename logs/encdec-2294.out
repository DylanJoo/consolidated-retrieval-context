[2025-06-04 16:54:13,675] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0604 16:54:16.441000 273237 site-packages/torch/distributed/run.py:766] 
W0604 16:54:16.441000 273237 site-packages/torch/distributed/run.py:766] *****************************************
W0604 16:54:16.441000 273237 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0604 16:54:16.441000 273237 site-packages/torch/distributed/run.py:766] *****************************************
[2025-06-04 16:54:20,704] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-04 16:54:20,704] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/hltcoe/jhueiju/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/hltcoe/jhueiju/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-06-04 16:54:22,641] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-06-04 16:54:22,643] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-06-04 16:54:22,643] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/home/hltcoe/jhueiju/.conda/envs/crc/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
2025-06-04 16:54:22,698 - INFO - Loading google/flan-t5-small (fid)
/home/hltcoe/jhueiju/.conda/envs/crc/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
2025-06-04 16:54:22,783 - INFO - Loading google/flan-t5-small (fid)
Some weights of FiDT5 were not initialized from the model checkpoint at google/flan-t5-small and are newly initialized: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of FiDT5 were not initialized from the model checkpoint at google/flan-t5-small and are newly initialized: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-06-04 16:54:23,376 - INFO - Finish loading in 0.59 sec.
2025-06-04 16:54:23,379 - INFO - Finish loading in 0.68 sec.
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
/home/hltcoe/jhueiju/.conda/envs/crc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/home/hltcoe/jhueiju/.conda/envs/crc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 533 examples [00:00, 1014.96 examples/s]Generating train split: 1069 examples [00:00, 1750.19 examples/s]Generating train split: 2148 examples [00:00, 2958.79 examples/s]Generating train split: 2676 examples [00:01, 3309.54 examples/s]Generating train split: 4308 examples [00:01, 5212.77 examples/s]Generating train split: 5000 examples [00:01, 3691.33 examples/s]
Filter:   0%|          | 0/5000 [00:00<?, ? examples/s]Filter:   0%|          | 0/5000 [00:00<?, ? examples/s]Filter:  80%|████████  | 4000/5000 [00:00<00:00, 34320.89 examples/s]Filter:  80%|████████  | 4000/5000 [00:00<00:00, 32704.83 examples/s]Filter: 100%|██████████| 5000/5000 [00:00<00:00, 34492.18 examples/s]
Filter: 100%|██████████| 5000/5000 [00:00<00:00, 33084.37 examples/s]
max_steps is given, it will override any value given in num_train_epochs
max_steps is given, it will override any value given in num_train_epochs
